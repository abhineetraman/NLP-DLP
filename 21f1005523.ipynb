{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":93282,"databundleVersionId":11098970,"sourceType":"competition"},{"sourceId":104449,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":68809,"modelId":91102}],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:24:26.003834Z","iopub.execute_input":"2025-02-16T17:24:26.004170Z","iopub.status.idle":"2025-02-16T17:24:26.912722Z","shell.execute_reply.started":"2025-02-16T17:24:26.004141Z","shell.execute_reply":"2025-02-16T17:24:26.911982Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/llama-3.1/transformers/8b-instruct/2/model.safetensors.index.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/model-00003-of-00004.safetensors\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/config.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/LICENSE\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/model-00001-of-00004.safetensors\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/README.md\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/USE_POLICY.md\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/tokenizer.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/tokenizer_config.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/model-00004-of-00004.safetensors\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/special_tokens_map.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/.gitattributes\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/model-00002-of-00004.safetensors\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/generation_config.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/original/consolidated.00.pth\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/original/params.json\n/kaggle/input/llama-3.1/transformers/8b-instruct/2/original/tokenizer.model\n/kaggle/input/multi-lingual-sentiment-analysis/sample_submission.csv\n/kaggle/input/multi-lingual-sentiment-analysis/train.csv\n/kaggle/input/multi-lingual-sentiment-analysis/test.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Basic Downloads","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install bitsandbytes\n!pip install accelerate\n!pip install peft\n!pip install evaluate\n!pip install --upgrade transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:24:26.913669Z","iopub.execute_input":"2025-02-16T17:24:26.914068Z","iopub.status.idle":"2025-02-16T17:24:54.582179Z","shell.execute_reply.started":"2025-02-16T17:24:26.914042Z","shell.execute_reply":"2025-02-16T17:24:54.581026Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from datasets import load_from_disk, load_dataset\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom peft import prepare_model_for_kbit_training, LoraConfig, PeftModel, get_peft_model\nimport torch\nfrom rich import print as rprint","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:24:54.584184Z","iopub.execute_input":"2025-02-16T17:24:54.584543Z","iopub.status.idle":"2025-02-16T17:25:16.599600Z","shell.execute_reply.started":"2025-02-16T17:24:54.584496Z","shell.execute_reply":"2025-02-16T17:25:16.598934Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"dataset = load_dataset(\"csv\", data_files=\"/kaggle/input/multi-lingual-sentiment-analysis/train.csv\")\n\ndataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:25:16.600897Z","iopub.execute_input":"2025-02-16T17:25:16.601581Z","iopub.status.idle":"2025-02-16T17:25:16.940212Z","shell.execute_reply.started":"2025-02-16T17:25:16.601546Z","shell.execute_reply":"2025-02-16T17:25:16.939497Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47df278d2d124b8082f08132921cdaa9"}},"metadata":{}},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['ID', 'sentence', 'label', 'language'],\n        num_rows: 1000\n    })\n})"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"from collections import Counter\n\n# Iterate over each dataset split (train, test, validation, etc.)\nfor split, ds in dataset.items():\n    rprint(f\"=== Unique value counts for {split} set ===\\n\")\n    columns = ['label', 'language']\n    for column in columns:\n        unique_counts = Counter(ds[column])  # Count occurrences of each unique value\n        rprint(f\"Column: {column}, Unique values: {len(unique_counts)}\")\n        rprint(f\"Sample values: {dict(list(unique_counts.items()))}\")  # Print first 5 unique values\n    print(\"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:25:16.941245Z","iopub.execute_input":"2025-02-16T17:25:16.941583Z","iopub.status.idle":"2025-02-16T17:25:16.960429Z","shell.execute_reply.started":"2025-02-16T17:25:16.941552Z","shell.execute_reply":"2025-02-16T17:25:16.959654Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"=== Unique value counts for train set ===\n\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">=== Unique value counts for train set ===\n\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Column: label, Unique values: \u001b[1;36m2\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Column: label, Unique values: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Sample values: \u001b[1m{\u001b[0m\u001b[32m'Positive'\u001b[0m: \u001b[1;36m507\u001b[0m, \u001b[32m'Negative'\u001b[0m: \u001b[1;36m493\u001b[0m\u001b[1m}\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Sample values: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Positive'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">507</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Negative'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">493</span><span style=\"font-weight: bold\">}</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Column: language, Unique values: \u001b[1;36m13\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Column: language, Unique values: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Sample values: \u001b[1m{\u001b[0m\u001b[32m'bn'\u001b[0m: \u001b[1;36m77\u001b[0m, \u001b[32m'gu'\u001b[0m: \u001b[1;36m77\u001b[0m, \u001b[32m'ta'\u001b[0m: \u001b[1;36m77\u001b[0m, \u001b[32m'pa'\u001b[0m: \u001b[1;36m77\u001b[0m, \u001b[32m'bd'\u001b[0m: \u001b[1;36m77\u001b[0m, \u001b[32m'as'\u001b[0m: \u001b[1;36m77\u001b[0m, \u001b[32m'te'\u001b[0m: \u001b[1;36m77\u001b[0m, \u001b[32m'or'\u001b[0m: \u001b[1;36m77\u001b[0m, \u001b[32m'ml'\u001b[0m: \u001b[1;36m76\u001b[0m, \u001b[32m'hi'\u001b[0m: \u001b[1;36m77\u001b[0m,\n\u001b[32m'mr'\u001b[0m: \u001b[1;36m77\u001b[0m, \u001b[32m'ur'\u001b[0m: \u001b[1;36m77\u001b[0m, \u001b[32m'kn'\u001b[0m: \u001b[1;36m77\u001b[0m\u001b[1m}\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Sample values: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'bn'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">77</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'gu'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">77</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'ta'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">77</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'pa'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">77</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'bd'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">77</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'as'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">77</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'te'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">77</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'or'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">77</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'ml'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">76</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'hi'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">77</span>,\n<span style=\"color: #008000; text-decoration-color: #008000\">'mr'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">77</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'ur'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">77</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'kn'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">77</span><span style=\"font-weight: bold\">}</span>\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"\n\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:25:16.961109Z","iopub.execute_input":"2025-02-16T17:25:16.961385Z","iopub.status.idle":"2025-02-16T17:25:16.967215Z","shell.execute_reply.started":"2025-02-16T17:25:16.961358Z","shell.execute_reply":"2025-02-16T17:25:16.966398Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['ID', 'sentence', 'label', 'language'],\n        num_rows: 1000\n    })\n})"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"def fix_labels(example):\n    label_map = {\"Positive\": 1, \"Negative\": 0}  # Map labels to integers\n    example[\"label\"] = label_map.get(example[\"label\"], -1)  # Assign -1 for unknown labels\n    return example\n\ndataset = dataset.map(fix_labels)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:25:16.968147Z","iopub.execute_input":"2025-02-16T17:25:16.968445Z","iopub.status.idle":"2025-02-16T17:25:17.053649Z","shell.execute_reply.started":"2025-02-16T17:25:16.968416Z","shell.execute_reply":"2025-02-16T17:25:17.052735Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9469c4c859da44859da2b069faf34dcb"}},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"## Loading the model and configuring it.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM\nfrom transformers import DataCollatorWithPadding\nfrom transformers import LlamaConfig, LlamaForCausalLM,LlamaForSequenceClassification\nfrom transformers import TrainingArguments, Trainer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:25:17.055632Z","iopub.execute_input":"2025-02-16T17:25:17.055822Z","iopub.status.idle":"2025-02-16T17:25:18.092989Z","shell.execute_reply.started":"2025-02-16T17:25:17.055805Z","shell.execute_reply":"2025-02-16T17:25:18.092384Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# # Quantization configuration\n# model_path = \"/kaggle/input/llama-3.1/transformers/8b-instruct/2\"\n\n# bnb_config = BitsAndBytesConfig(\n#     load_in_4bit=True,\n#     bnb_4bit_use_double_quant=False,\n#     bnb_4bit_quant_type=\"nf4\",\n#     bnb_4bit_compute_dtype=torch.bfloat16,\n# )\n\n# # Loading the model and tokenizer\n\n# model = AutoModelForCausalLM.from_pretrained(model_path,quantization_config=bnb_config,\n#                                              device_map=\"auto\")\n# tokenizer = AutoTokenizer.from_pretrained(\n#     model_path,\n#     model_max_length=1024,\n#     padding_side=\"left\",\n#     add_eos_token=True)\n# tokenizer.pad_token = tokenizer.eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:25:18.094132Z","iopub.execute_input":"2025-02-16T17:25:18.094342Z","iopub.status.idle":"2025-02-16T17:25:18.097606Z","shell.execute_reply.started":"2025-02-16T17:25:18.094324Z","shell.execute_reply":"2025-02-16T17:25:18.096871Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"model_id = \"/kaggle/input/llama-3.1/transformers/8b-instruct/2\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_id, model_max_length=1024)\n# set pad token id\ntokenizer.pad_token=tokenizer.eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:25:18.098345Z","iopub.execute_input":"2025-02-16T17:25:18.098586Z","iopub.status.idle":"2025-02-16T17:25:18.714406Z","shell.execute_reply.started":"2025-02-16T17:25:18.098567Z","shell.execute_reply":"2025-02-16T17:25:18.713731Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def tokenize(example):\n    return tokenizer(example[\"sentence\"], padding=True, truncation=True, max_length=512)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:25:18.715205Z","iopub.execute_input":"2025-02-16T17:25:18.715498Z","iopub.status.idle":"2025-02-16T17:25:18.719494Z","shell.execute_reply.started":"2025-02-16T17:25:18.715469Z","shell.execute_reply":"2025-02-16T17:25:18.718827Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"tokenized_ds = dataset.map(tokenize, batched=True, num_proc=4, remove_columns=['sentence'])\nprint(tokenized_ds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:25:18.720277Z","iopub.execute_input":"2025-02-16T17:25:18.720577Z","iopub.status.idle":"2025-02-16T17:25:19.482581Z","shell.execute_reply.started":"2025-02-16T17:25:18.720539Z","shell.execute_reply":"2025-02-16T17:25:19.481758Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6a1d05a533841fba46662897f2b40d7"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['ID', 'label', 'language', 'input_ids', 'attention_mask'],\n        num_rows: 1000\n    })\n})\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"ds_split = tokenized_ds['train'].train_test_split(test_size=0.07,seed=50)\nprint(ds_split)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:25:19.483468Z","iopub.execute_input":"2025-02-16T17:25:19.483706Z","iopub.status.idle":"2025-02-16T17:25:19.496399Z","shell.execute_reply.started":"2025-02-16T17:25:19.483683Z","shell.execute_reply":"2025-02-16T17:25:19.495709Z"}},"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['ID', 'label', 'language', 'input_ids', 'attention_mask'],\n        num_rows: 930\n    })\n    test: Dataset({\n        features: ['ID', 'label', 'language', 'input_ids', 'attention_mask'],\n        num_rows: 70\n    })\n})\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"# Data Collator","metadata":{}},{"cell_type":"code","source":"data_collator = DataCollatorWithPadding(tokenizer,padding='max_length', max_length=512)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:25:19.497172Z","iopub.execute_input":"2025-02-16T17:25:19.497453Z","iopub.status.idle":"2025-02-16T17:25:19.511216Z","shell.execute_reply.started":"2025-02-16T17:25:19.497421Z","shell.execute_reply":"2025-02-16T17:25:19.510335Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"import torch, gc\n# gc.collect()\n# torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:25:19.512047Z","iopub.execute_input":"2025-02-16T17:25:19.512298Z","iopub.status.idle":"2025-02-16T17:25:19.525533Z","shell.execute_reply.started":"2025-02-16T17:25:19.512272Z","shell.execute_reply":"2025-02-16T17:25:19.524747Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"bnb_config = BitsAndBytesConfig(\n    load_in_4bit=True, \n    llm_int8_enable_fp32_cpu_offload=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n)\n\nmodel = AutoModelForSequenceClassification.from_pretrained(model_id,\n                                                           num_labels=2,\n                                                           pad_token_id=tokenizer.eos_token_id,\n                                                           quantization_config=bnb_config,\n                                                           device_map=\"auto\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:25:19.526457Z","iopub.execute_input":"2025-02-16T17:25:19.526692Z","iopub.status.idle":"2025-02-16T17:26:22.579412Z","shell.execute_reply.started":"2025-02-16T17:25:19.526674Z","shell.execute_reply":"2025-02-16T17:26:22.578695Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1636cda632de463e95365e89519cf4e2"}},"metadata":{}},{"name":"stderr","text":"Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/llama-3.1/transformers/8b-instruct/2 and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"import evaluate\nmetric = evaluate.load(\"f1\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels, average=\"macro\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:26:22.580150Z","iopub.execute_input":"2025-02-16T17:26:22.580373Z","iopub.status.idle":"2025-02-16T17:26:23.216759Z","shell.execute_reply.started":"2025-02-16T17:26:22.580353Z","shell.execute_reply":"2025-02-16T17:26:23.216141Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.79k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac48d2c3f2f543c5af5ebd1ebed37e3f"}},"metadata":{}}],"execution_count":17},{"cell_type":"markdown","source":"# Checking Models and Tokenizers","metadata":{}},{"cell_type":"code","source":"print(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:26:23.217596Z","iopub.execute_input":"2025-02-16T17:26:23.217837Z","iopub.status.idle":"2025-02-16T17:26:23.223909Z","shell.execute_reply.started":"2025-02-16T17:26:23.217817Z","shell.execute_reply":"2025-02-16T17:26:23.223067Z"}},"outputs":[{"name":"stdout","text":"LlamaForSequenceClassification(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(128256, 4096, padding_idx=128009)\n    (layers): ModuleList(\n      (0-31): 32 x LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (score): Linear(in_features=4096, out_features=2, bias=False)\n)\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"# Fine Tune with LoRA","metadata":{}},{"cell_type":"code","source":"#lora\nfrom peft import LoraConfig, TaskType, LoraModel\nlora_config = LoraConfig(\n    r=16,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    task_type=TaskType.SEQ_CLS,\n    inference_mode=False,\n    lora_alpha=32,\n    lora_dropout=0.05\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:26:23.224824Z","iopub.execute_input":"2025-02-16T17:26:23.225123Z","iopub.status.idle":"2025-02-16T17:26:23.241631Z","shell.execute_reply.started":"2025-02-16T17:26:23.225102Z","shell.execute_reply":"2025-02-16T17:26:23.240945Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"from peft import get_peft_model\nmodel = prepare_model_for_kbit_training(model)\nlora_model = get_peft_model(model, lora_config)\nlora_model.print_trainable_parameters()\nprint(lora_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:26:23.242329Z","iopub.execute_input":"2025-02-16T17:26:23.242597Z","iopub.status.idle":"2025-02-16T17:26:23.448684Z","shell.execute_reply.started":"2025-02-16T17:26:23.242564Z","shell.execute_reply":"2025-02-16T17:26:23.447964Z"}},"outputs":[{"name":"stdout","text":"trainable params: 6,823,936 || all params: 7,511,756,800 || trainable%: 0.0908\nPeftModelForSequenceClassification(\n  (base_model): LoraModel(\n    (model): LlamaForSequenceClassification(\n      (model): LlamaModel(\n        (embed_tokens): Embedding(128256, 4096, padding_idx=128009)\n        (layers): ModuleList(\n          (0-31): 32 x LlamaDecoderLayer(\n            (self_attn): LlamaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n            )\n            (mlp): LlamaMLP(\n              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n              (act_fn): SiLU()\n            )\n            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n          )\n        )\n        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n        (rotary_emb): LlamaRotaryEmbedding()\n      )\n      (score): ModulesToSaveWrapper(\n        (original_module): Linear(in_features=4096, out_features=2, bias=False)\n        (modules_to_save): ModuleDict(\n          (default): Linear(in_features=4096, out_features=2, bias=False)\n        )\n      )\n    )\n  )\n)\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"lora_model.peft_config","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:26:23.449461Z","iopub.execute_input":"2025-02-16T17:26:23.449767Z","iopub.status.idle":"2025-02-16T17:26:23.454803Z","shell.execute_reply.started":"2025-02-16T17:26:23.449738Z","shell.execute_reply":"2025-02-16T17:26:23.454101Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"{'default': LoraConfig(task_type=<TaskType.SEQ_CLS: 'SEQ_CLS'>, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='/kaggle/input/llama-3.1/transformers/8b-instruct/2', revision=None, inference_mode=False, r=16, target_modules={'v_proj', 'q_proj'}, exclude_modules=None, lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['classifier', 'score'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, eva_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)}"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"training_args = TrainingArguments( output_dir='lora_llama_8b_ct',\n                                  eval_strategy=\"steps\",\n                                  eval_steps=50,\n                                  num_train_epochs=2,\n                                  per_device_train_batch_size=4,\n                                  per_device_eval_batch_size=4,\n                                  bf16=False,\n                                  fp16=True,\n                                  tf32=False,\n                                  gradient_accumulation_steps=2,\n                                  adam_beta1=0.05,\n                                  adam_beta2=0.995,\n                                  learning_rate=1e-4,\n                                  weight_decay=0.02,\n                                  logging_dir='logs',\n                                  logging_strategy=\"steps\",\n                                  logging_steps = 50,\n                                  save_steps=50,\n                                  save_total_limit=20,\n                                  report_to='none',\n                                  half_precision_backend = 'amp',\n                                  load_best_model_at_end = True,\n                                  #use_reentrant=True\n                                )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:26:23.455604Z","iopub.execute_input":"2025-02-16T17:26:23.455805Z","iopub.status.idle":"2025-02-16T17:26:23.497811Z","shell.execute_reply.started":"2025-02-16T17:26:23.455787Z","shell.execute_reply":"2025-02-16T17:26:23.497202Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"trainer = Trainer(model=lora_model,\n                  args = training_args,\n                  train_dataset=ds_split[\"train\"],\n                  eval_dataset=ds_split[\"test\"],\n                  compute_metrics = compute_metrics,\n                  data_collator = data_collator)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:26:23.500594Z","iopub.execute_input":"2025-02-16T17:26:23.500797Z","iopub.status.idle":"2025-02-16T17:26:23.516632Z","shell.execute_reply.started":"2025-02-16T17:26:23.500780Z","shell.execute_reply":"2025-02-16T17:26:23.515785Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"results = trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:26:23.517800Z","iopub.execute_input":"2025-02-16T17:26:23.518042Z","iopub.status.idle":"2025-02-16T18:24:24.723321Z","shell.execute_reply.started":"2025-02-16T17:26:23.517990Z","shell.execute_reply":"2025-02-16T18:24:24.722472Z"}},"outputs":[{"name":"stderr","text":"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='232' max='232' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [232/232 57:47, Epoch 1/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>1.341000</td>\n      <td>0.695504</td>\n      <td>0.668110</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.422700</td>\n      <td>0.309561</td>\n      <td>0.914005</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.222700</td>\n      <td>0.340995</td>\n      <td>0.928440</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.250800</td>\n      <td>0.296722</td>\n      <td>0.928557</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"# Let's Evaluate","metadata":{}},{"cell_type":"code","source":"test_dataset = load_dataset(\"csv\", data_files=\"/kaggle/input/multi-lingual-sentiment-analysis/test.csv\")\n\ntest_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T18:24:24.724253Z","iopub.execute_input":"2025-02-16T18:24:24.724586Z","iopub.status.idle":"2025-02-16T18:24:24.988824Z","shell.execute_reply.started":"2025-02-16T18:24:24.724551Z","shell.execute_reply":"2025-02-16T18:24:24.988185Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e629c602ba714698b1782f64d5c09bf2"}},"metadata":{}},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['ID', 'sentence', 'language'],\n        num_rows: 100\n    })\n})"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"sample_sub = load_dataset(\"csv\", data_files=\"/kaggle/input/multi-lingual-sentiment-analysis/sample_submission.csv\")\n\nsample_sub['train'][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T18:24:24.989553Z","iopub.execute_input":"2025-02-16T18:24:24.989791Z","iopub.status.idle":"2025-02-16T18:24:25.102139Z","shell.execute_reply.started":"2025-02-16T18:24:24.989760Z","shell.execute_reply":"2025-02-16T18:24:25.101505Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a941ebf22124ee4996e3421b0b1b393"}},"metadata":{}},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"{'ID': 1, 'label': 'Positive'}"},"metadata":{}}],"execution_count":26},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"from transformers import TextClassificationPipeline\nclassifier = TextClassificationPipeline(model=model,\n                                       tokenizer=tokenizer,\n                                       framework='pt',\n                                       task=\"sentiment-analysis\",\n                                       #device = \"cuda\"\n                                       )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T18:24:25.102870Z","iopub.execute_input":"2025-02-16T18:24:25.103089Z","iopub.status.idle":"2025-02-16T18:24:25.108303Z","shell.execute_reply.started":"2025-02-16T18:24:25.103070Z","shell.execute_reply":"2025-02-16T18:24:25.107382Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"model.config.id2label = {0:\"Negative\",1:\"Positive\"}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T18:24:25.109210Z","iopub.execute_input":"2025-02-16T18:24:25.109526Z","iopub.status.idle":"2025-02-16T18:24:25.126593Z","shell.execute_reply.started":"2025-02-16T18:24:25.109497Z","shell.execute_reply":"2025-02-16T18:24:25.125732Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"sample = test_dataset['train'][0]['sentence']\nprint(f\"Sample: {sample}\")\nprediction = classifier(sample)\nprint(prediction)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T18:24:25.127405Z","iopub.execute_input":"2025-02-16T18:24:25.127631Z","iopub.status.idle":"2025-02-16T18:24:25.914781Z","shell.execute_reply.started":"2025-02-16T18:24:25.127598Z","shell.execute_reply":"2025-02-16T18:24:25.914101Z"}},"outputs":[{"name":"stdout","text":"Sample: 1120 mAh, ਓਵਰਚਾਰਜਿੰਗ ਦੀ ਸੁਰੱਖਿਆ\n[{'label': 'Positive', 'score': 0.9206250905990601}]\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"# Extract text data and ID from test_dataset (DatasetDict format)\ntest_texts = test_dataset[\"train\"][\"sentence\"]  # Adjust key if needed\ntest_ids = test_dataset[\"train\"][\"ID\"]  # Existing ID column","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T18:24:25.915799Z","iopub.execute_input":"2025-02-16T18:24:25.916148Z","iopub.status.idle":"2025-02-16T18:24:25.920502Z","shell.execute_reply.started":"2025-02-16T18:24:25.916117Z","shell.execute_reply":"2025-02-16T18:24:25.919655Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# Run inference using the pipeline\npredictions = classifier(test_texts, batch_size=32)  # Batched for efficiency\n\n# Convert pipeline output to labels\npredicted_labels = [pred[\"label\"] for pred in predictions]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T18:24:25.921405Z","iopub.execute_input":"2025-02-16T18:24:25.921655Z","iopub.status.idle":"2025-02-16T18:27:59.585115Z","shell.execute_reply.started":"2025-02-16T18:24:25.921635Z","shell.execute_reply":"2025-02-16T18:27:59.584089Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"# Save results to CSV\noutput_df = pd.DataFrame({\"ID\": test_ids, \"label\": predicted_labels})\noutput_df.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T18:27:59.586069Z","iopub.execute_input":"2025-02-16T18:27:59.586341Z","iopub.status.idle":"2025-02-16T18:27:59.596362Z","shell.execute_reply.started":"2025-02-16T18:27:59.586309Z","shell.execute_reply":"2025-02-16T18:27:59.595595Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"print(output_df.head(10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T18:27:59.597212Z","iopub.execute_input":"2025-02-16T18:27:59.597453Z","iopub.status.idle":"2025-02-16T18:27:59.614574Z","shell.execute_reply.started":"2025-02-16T18:27:59.597423Z","shell.execute_reply":"2025-02-16T18:27:59.613783Z"}},"outputs":[{"name":"stdout","text":"   ID     label\n0   1  Positive\n1   2  Positive\n2   3  Positive\n3   4  Positive\n4   5  Negative\n5   6  Negative\n6   7  Positive\n7   8  Positive\n8   9  Negative\n9  10  Positive\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"sample = test_dataset['train'][11]['sentence']\nprint(f\"Sample: {sample}\")\nprediction = classifier(sample)\nprint(prediction)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T18:27:59.615339Z","iopub.execute_input":"2025-02-16T18:27:59.615645Z","iopub.status.idle":"2025-02-16T18:28:01.121801Z","shell.execute_reply.started":"2025-02-16T18:27:59.615617Z","shell.execute_reply":"2025-02-16T18:28:01.121076Z"}},"outputs":[{"name":"stdout","text":"Sample: పాత బాడీ షేమింగ్ జోక్‌లు, మమ్మల్ని నవ్వించడానికి చాలా కష్టపడతాయి. ట్రైలర్ చూస్తే సినిమా గురించి ఇప్పటికే 90% తెలిసిపోతుంది\n[{'label': 'Negative', 'score': 0.9348655939102173}]\n","output_type":"stream"}],"execution_count":34}]}